{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cf3e997c-1878-4bc7-be79-1bbb96e31a9d",
    "_uuid": "e32c6893aff5e81e29671bb63836a78d4c1f4f2c"
   },
   "source": [
    "Hi all,\n",
    "\n",
    "This is my first Kernel on Kaggle.\n",
    "\n",
    "I will try to build a rough model using Gaussian Distribution to detect Anamolous transactions.\n",
    "\n",
    "**Reason behind using Gaussian Distribution:-**  <br>\n",
    "If I can summarize what Andrew Ng has mentioned in his lecture on Anomaly detection is \n",
    "Supervised Classification technique is not the perfect candidate for highly imbalanced data. In this case it is \n",
    " 0.172% (near to 0)\n",
    "\n",
    "If We think from the persepctive of building the model to find out the anomalous data which is not seen very frequently \n",
    "We should go for Anomaly detection technique using Gaussian Distribution.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "8ba10cf2-fea4-49fb-a5d0-80412d3cbcae",
    "_uuid": "bae1f5ea58db298919959facea375d58c379b043"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5444215ceead>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision_recall_curve\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_objs\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moffline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdownload_plotlyjs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minit_notebook_mode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0miplot\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0minit_notebook_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnected\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from numpy import genfromtxt\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score , average_precision_score\n",
    "from sklearn.metrics import precision_score, precision_recall_curve\n",
    "%matplotlib inline\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs,init_notebook_mode,plot,iplot\n",
    "init_notebook_mode(connected=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bfa0c373-17b9-48a1-b8f1-4fcc831b20fd",
    "_uuid": "be9fdf7a34980b73d70488b23c8b766fa14e9f80"
   },
   "source": [
    "I will be defining the below two functions which are required to calculate Gaussian Distribution of the normalized variables provided in the dataset (V1, V2 ....V28, Amount ).  <br>\n",
    "note- These functions will be invoked for building the model\n",
    "\n",
    "1) Find out mu and Sigma for the dataframe variables passed to this function. <br>\n",
    "      ----\n",
    "2) Calculate Probability Distribution for the each row (I will explain why we need Probality for each row as we proceed) <br>\n",
    "       ----\n",
    "       \n",
    "Formula:- \n",
    "if each example x has N dimensiona(features) then below formula is used to calculate the P value <br>\n",
    "**P(x) = p(x1,u1,sigma1^2)p(x2,u2,sigma2^2)p(x3,u3,sigma3^2).....p(xn,un,sigma'N'^2)**\n",
    "      ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f9b5bb38-710d-4166-803e-8a4564516540",
    "_uuid": "78b33b829a9cd3745b6340026bf197d8a0d1f3c4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimateGaussian(dataset):\n",
    "    mu = np.mean(dataset, axis=0)\n",
    "    sigma = np.cov(dataset.T)\n",
    "    return mu, sigma\n",
    "\n",
    "def multivariateGaussian(dataset,mu,sigma):\n",
    "    p = multivariate_normal(mean=mu, cov=sigma)\n",
    "    return p.pdf(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "376a04fb-9285-48cc-920f-4fe71966a878",
    "_uuid": "0d7e27320f5b6982a05efdb552c927a282781e53"
   },
   "source": [
    "Below is the most crucial function used to detect how well we are doing with our subset (Cross validation subset) .\n",
    "I have decided values for Epsilon for detecting the fradulent transactions from the Subsets.  <br><br>\n",
    "**(Tip :- Ideally you should provide range of epsilon values, due to time constraint on running this kernel i have provided few values here for demonstration purpose)**\n",
    "\n",
    " **For now remember Epsilon value is the threshold value below which we will mark transaction as Anomalous.**\n",
    "           ----\n",
    "\n",
    "Rewriting above sentense again \n",
    "P(x) for X if less than the epsilon value then mark that transaction as anomalous transaction. \n",
    "\n",
    "We need to maintain healthy balance between the Recall and Precision . We may get Recall value above 0.80 and close to 0.90 here but at the expense of reducing our precision which is not advisable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "09caa539-d1f2-499d-b00b-5532e6002f14",
    "_uuid": "6aaf6c9af7c49177bd4da13d85d082ab0e57984b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selectThresholdByCV(probs,gt):\n",
    "    best_epsilon = 0\n",
    "    best_f1 = 0\n",
    "    f = 0\n",
    "    farray = []\n",
    "    Recallarray = []\n",
    "    Precisionarray = []\n",
    "    epsilons = (0.0000e+00, 1.0527717316e-70, 1.0527717316e-50, 1.0527717316e-24)\n",
    "    #epsilons = np.asarray(epsilons)\n",
    "    for epsilon in epsilons:\n",
    "        predictions = (p_cv < epsilon)\n",
    "        f = f1_score(train_cv_y, predictions, average = \"binary\")\n",
    "        Recall = recall_score(train_cv_y, predictions, average = \"binary\")\n",
    "        Precision = precision_score(train_cv_y, predictions, average = \"binary\")\n",
    "        farray.append(f)\n",
    "        Recallarray.append(Recall)\n",
    "        Precisionarray.append(Precision)\n",
    "        print ('For below Epsilon')\n",
    "        print(epsilon)\n",
    "        print ('F1 score , Recall and Precision are as below')\n",
    "        print ('Best F1 Score %f' %f)\n",
    "        print ('Best Recall Score %f' %Recall)\n",
    "        print ('Best Precision Score %f' %Precision)\n",
    "        print ('-'*40)\n",
    "        if f > best_f1:\n",
    "            best_f1 = f\n",
    "            best_recall = Recall\n",
    "            best_precision = Precision\n",
    "            best_epsilon = epsilon    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0.1, 0.5, 0.7, 0.3])\n",
    "    #plt.subplot(3,1,1)\n",
    "    plt.plot(farray ,\"ro\")\n",
    "    plt.plot(farray)\n",
    "    ax.set_xticks(range(5))\n",
    "    ax.set_xticklabels(epsilons,rotation = 60 ,fontsize = 'medium' )\n",
    "    ax.set_ylim((0,0.8))\n",
    "    ax.set_title('F1 score vs Epsilon value')\n",
    "    ax.annotate('Best F1 Score', xy=(best_epsilon,best_f1), xytext=(best_epsilon,best_f1))\n",
    "    plt.xlabel(\"Epsilon value\") \n",
    "    plt.ylabel(\"F1 Score\") \n",
    "    plt.show()\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0.1, 0.5, 0.9, 0.3])\n",
    "    #plt.subplot(3,1,2)\n",
    "    plt.plot(Recallarray ,\"ro\")\n",
    "    plt.plot(Recallarray)\n",
    "    ax.set_xticks(range(5))\n",
    "    ax.set_xticklabels(epsilons,rotation = 60 ,fontsize = 'medium' )\n",
    "    ax.set_ylim((0,1.0))\n",
    "    ax.set_title('Recall vs Epsilon value')\n",
    "    ax.annotate('Best Recall Score', xy=(best_epsilon,best_recall), xytext=(best_epsilon,best_recall))\n",
    "    plt.xlabel(\"Epsilon value\") \n",
    "    plt.ylabel(\"Recall Score\") \n",
    "    plt.show()\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0.1, 0.5, 0.9, 0.3])\n",
    "    #plt.subplot(3,1,3)\n",
    "    plt.plot(Precisionarray ,\"ro\")\n",
    "    plt.plot(Precisionarray)\n",
    "    ax.set_xticks(range(5))\n",
    "    ax.set_xticklabels(epsilons,rotation = 60 ,fontsize = 'medium' )\n",
    "    ax.set_ylim((0,0.8))\n",
    "    ax.set_title('Precision vs Epsilon value')\n",
    "    ax.annotate('Best Precision Score', xy=(best_epsilon,best_precision), xytext=(best_epsilon,best_precision))\n",
    "    plt.xlabel(\"Epsilon value\") \n",
    "    plt.ylabel(\"Precision Score\") \n",
    "    plt.show()\n",
    "    return best_f1, best_epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f765fc12-530b-4829-a82d-852323f945f6",
    "_uuid": "f897b22bc44b2f1650a2830a2ea01764357ce4db"
   },
   "source": [
    "Lets Read the dataset \n",
    "        ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e0707e73-37f4-422d-a961-71a51b0dd42e",
    "_uuid": "a74a641e3836e1bd8c5b5cd4ecf8fcacf14928f6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input//creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1706fc65-8907-44e4-92b4-9c8519168e19",
    "_uuid": "a249d0c35eb6bfdca826b356636576995d265cc8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(train_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7c9b9fcc-fbbf-4601-b530-898d57d32882",
    "_uuid": "6708840763cd7cffb17632c91f59d38ef0d59b01"
   },
   "source": [
    "**Copied below piece of code for visualization from the kernel shared by the expert to identify which features are not much of help in the algorithm. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1e157f13-827b-47bd-bd3b-14ab15e77366",
    "_uuid": "2a1e5d225e6bc9e7296462de4525b671e5e4cb6d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_features = train_df.iloc[:,1:29].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "be34dc83-a4e0-4757-9a9d-54e6d0bb67e9",
    "_uuid": "587eb7aae36154d75461e79b7c131eed8c8976a6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8*4))\n",
    "gs = gridspec.GridSpec(7, 4)\n",
    "for i, cn in enumerate(train_df[v_features]):\n",
    "    ax = plt.subplot(gs[i])\n",
    "    sns.distplot(train_df[cn][train_df.Class == 1], bins=50)\n",
    "    sns.distplot(train_df[cn][train_df.Class == 0], bins=50)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_title('feature: ' + str(cn))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "233b3258-df03-4ff9-a770-56b618fcaf9c",
    "_uuid": "fa7c9cce52d535265aed42cb15a362ccb2709b7a"
   },
   "source": [
    "We see normal distribution of anomalous transation is matching with normal distribution of Normal transaction for below Features \n",
    "'V28','V27','V26','V25','V24','V23','V22','V20','V15','V13','V8'\n",
    "Better we remove these features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "20d84ce3-e99a-44ba-b39e-6f24aa489afe",
    "_uuid": "d9cda40f61bfcc22ce81b1bbe39ca951f31e0350",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df.drop(['V28','V27','V26','V25','V24','V23','V22','V20','V15','V13','V8'], axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fcf8bea9-3325-4e49-b552-8fa8022807e9",
    "_uuid": "abccdc97bfe4603c4eedc92ea2fdc3a6b9308d08"
   },
   "source": [
    "I have removed Amount and Time feature since they wont add much value in calculating gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1e9c0a38-0536-4652-9cfa-3094a2bebf48",
    "_uuid": "8b434da9d51f825a467d60b99680f100e284fd6f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df.drop(labels = [\"Amount\",\"Time\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c23a9cd6-bc1d-4fd9-a786-d64ce4487f3a",
    "_uuid": "3c6668db06edcff208987fd08682be6a1c5802d8"
   },
   "source": [
    "Split the dataset into 2 part one with Class 1 and other with class 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ac4e594e-bb70-4496-b4d2-9357b80b99ad",
    "_uuid": "fe78e559d679120f6420a7b74d63ad9f92f0cbed",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_strip_v1 = train_df[train_df[\"Class\"] == 1]\n",
    "train_strip_v0 = train_df[train_df[\"Class\"] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "93659d18-d7f2-4ecc-83ba-bd4443217984",
    "_uuid": "cb1c7496fec012a39e16dd97ef970a07308f055f"
   },
   "source": [
    "In the Anomalized technique  we distribute this large dataset into 3 parts .\n",
    "\n",
    "1) Normal Transactons: classified as 0 , no anomalized transaction should be present here since it is not a supervised method<br>  How to get this dataset :- 60% of normal transactions should be added here. <br> \n",
    "Find out Epsilon by using  min(Probability) command \n",
    "\n",
    "2) dataset for Cross validation : from the remaining normal transaction take 50 % (i.e. 20 % as a whole since we have already took the data in the first step)  and add 50% of the Anomalized data with this .\n",
    "\n",
    "3) dataset for testing the algorithm :- this step is similar to what we did for Cross validattion. <br>\n",
    " Test dataset = leftover normal transaction + leftover Anomalized data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f6eef861-1ed0-4dc8-a98c-ce329b05b370",
    "_uuid": "ac713bf2ef332551825185dce57f84aa65e60dad",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Normal_len = len (train_strip_v0)\n",
    "Anomolous_len = len (train_strip_v1)\n",
    "\n",
    "start_mid = Anomolous_len // 2\n",
    "start_midway = start_mid + 1\n",
    "\n",
    "train_cv_v1  = train_strip_v1 [: start_mid]\n",
    "train_test_v1 = train_strip_v1 [start_midway:Anomolous_len]\n",
    "\n",
    "start_mid = (Normal_len * 60) // 100\n",
    "start_midway = start_mid + 1\n",
    "\n",
    "cv_mid = (Normal_len * 80) // 100\n",
    "cv_midway = cv_mid + 1\n",
    "\n",
    "train_fraud = train_strip_v0 [:start_mid]\n",
    "train_cv    = train_strip_v0 [start_midway:cv_mid]\n",
    "train_test  = train_strip_v0 [cv_midway:Normal_len]\n",
    "\n",
    "train_cv = pd.concat([train_cv,train_cv_v1],axis=0)\n",
    "train_test = pd.concat([train_test,train_test_v1],axis=0)\n",
    "\n",
    "\n",
    "print(train_fraud.columns.values)\n",
    "print(train_cv.columns.values)\n",
    "print(train_test.columns.values)\n",
    "\n",
    "train_cv_y = train_cv[\"Class\"]\n",
    "train_test_y = train_test[\"Class\"]\n",
    "\n",
    "train_cv.drop(labels = [\"Class\"], axis = 1, inplace = True)\n",
    "train_fraud.drop(labels = [\"Class\"], axis = 1, inplace = True)\n",
    "train_test.drop(labels = [\"Class\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bba5158d-47a5-4014-81c2-a137798e4cbf",
    "_uuid": "890be9349074ace71e60248ffd9ae5233415befe"
   },
   "source": [
    "Choosing Epsilon Values <br>\n",
    "    ---\n",
    "I calculated P value for all the rows present in Normal Transaction and found the minimum P value \n",
    "by using below command\n",
    " **min(p)** \n",
    "      ---\n",
    "similalrly I found the minimum P Value for rest of the datasets and found this value to be very close to 0 and then i found the max(p) value which is again somewhat far from 0. <br><br>\n",
    "Instead of looping between the epsilon values (between min and max of P) , i chose set of epsilon values for demonstration purpose to see how well i can perform to find the fraudulent transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "834c407e-47aa-40a4-9900-03ec7ef01ef2",
    "_uuid": "2dee73a4d60406fe5d833fc7573d4d43f8722060",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu, sigma = estimateGaussian(train_fraud)\n",
    "p = multivariateGaussian(train_fraud,mu,sigma)\n",
    "p_cv = multivariateGaussian(train_cv,mu,sigma)\n",
    "p_test = multivariateGaussian(train_test,mu,sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "09cb7cf7-8a57-42ad-bb5b-faddd6bd53c9",
    "_uuid": "1706a8a385b73bc8f8677e0d1ad51bb26599862a"
   },
   "source": [
    "Performance wrt to Epsilon values\n",
    "    ----\n",
    "Check out how well we are performing with the given set of epsilon values from the function called here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "43f5e9b4-8125-46ca-bdae-70d3add290b8",
    "_uuid": "8320b88b4d055aff7b0d5db54e4d22928bfa6c38",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fscore, ep= selectThresholdByCV(p_cv,train_cv_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d7593502-ef6a-4891-bf89-db49854332bd",
    "_uuid": "cd5399b33805a1b3fc07a9121aec1d2e709fed44",
    "collapsed": true
   },
   "source": [
    "Epsilon value = 1.0527717316e-70 is selected as threshold to identify Anomalous transactions \n",
    "\n",
    "now time to Predict and calculate  F1 , Recall and Precision score for our Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c60b8cca-6d84-47f8-91fd-08e7d161c2c9",
    "_uuid": "09d9efa434d6ebcfa9c4753840b55fdfdb102f3f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = (p_test < ep)\n",
    "Recall = recall_score(train_test_y, predictions, average = \"binary\")    \n",
    "Precision = precision_score(train_test_y, predictions, average = \"binary\")\n",
    "F1score = f1_score(train_test_y, predictions, average = \"binary\")    \n",
    "print ('F1 score , Recall and Precision for Test dataset')\n",
    "print ('Best F1 Score %f' %F1score)\n",
    "print ('Best Recall Score %f' %Recall)\n",
    "print ('Best Precision Score %f' %Precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "32263e59-72a7-45e9-b3a1-1be0a8821dfe",
    "_uuid": "94a1153ef19da1e38a7efefbcf680fc1d0224633"
   },
   "source": [
    "Lets Visualize our predictions in below scatter plot \n",
    "         -------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dbd3a061-c143-4e23-9992-d2be680f0509",
    "_uuid": "3c62c8c46eaf0b39c5f11b70c39ac689bfbcc3eb",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.scatter(train_test['V14'],train_test['V11'],marker=\"o\", color=\"lightBlue\")\n",
    "ax.set_title('Anomalies(in red) vs Predicted Anomalies(in Green)')\n",
    "for i, txt in enumerate(train_test['V14'].index):\n",
    "       if train_test_y.loc[txt] == 1 :\n",
    "            ax.annotate('*', (train_test['V14'].loc[txt],train_test['V11'].loc[txt]),fontsize=13,color='Red')\n",
    "       if predictions[i] == True :\n",
    "            ax.annotate('o', (train_test['V14'].loc[txt],train_test['V11'].loc[txt]),fontsize=15,color='Green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4b3f43b5-b021-4201-befa-2c6eaaff8fa1",
    "_uuid": "a4abe1dc06fa5b5d6e3bb8f08704671fe2205dc9"
   },
   "source": [
    "From the above result we can see that we are able to maintain the balance between Recall and Precision. \n",
    "\n",
    "Precision of around 60% with Recall of 74% is not bad at all when we have such highly unbalanced data. \n",
    "These numbers are not fixed and can vary . \n",
    " \n",
    " These numbers were different for Cross validation dataset and we shortlisted our Epsilon value by comparing the results of F1 Score.\n",
    "\n",
    "I will show you the result we achieved on Cross validation dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "00b86679-7e69-461e-a93b-a9cb82528912",
    "_uuid": "c0502af04356d265281f2d736e4c5f9d3b3d6744",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = (p_cv < ep)\n",
    "Recall = recall_score(train_cv_y, predictions, average = \"binary\")    \n",
    "Precision = precision_score(train_cv_y, predictions, average = \"binary\")\n",
    "F1score = f1_score(train_cv_y, predictions, average = \"binary\")    \n",
    "print ('F1 score , Recall and Precision for Cross Validation dataset')\n",
    "print ('Best F1 Score %f' %F1score)\n",
    "print ('Best Recall Score %f' %Recall)\n",
    "print ('Best Precision Score %f' %Precision)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6898077a-922a-44cc-9352-f26224793676",
    "_uuid": "6ee6a6495c9c5c891f6e250aa2e323e80551bf65"
   },
   "source": [
    "\n",
    " Summary of above Algorithm: \n",
    " \n",
    " 1) Find Epsilon value by considering only Normal Transaction.\n",
    " \n",
    " 2) Use this Epsilon value on CV dataset (Normal transaction + Anomalous transaction)\n",
    " \n",
    " 3) Come up with set of Epsilon values to see how your algorithm performs and note down the Best F1 score along with\n",
    "      Recall and Precision percentage \n",
    "      \n",
    " 4) Choose the Epsilon value with highest F1 score \n",
    " \n",
    " 5) Use this Epsilon value to predict the Anomalous transaction on Test Dataset   \n",
    " \n",
    "Please comment and let me know to help improve this kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "07750393-1be0-4c53-9a84-a9fd06f9f497",
    "_uuid": "7cb720ed036a0c96a17fac0d73c7e6b2c0088e5f"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "20c81f4a-463c-4698-b9b2-d206a42d71aa",
    "_uuid": "c9059245eaee9da475209aed05aa3d2eb20f0e6f"
   },
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
